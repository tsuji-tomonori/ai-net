---
title_en: "The Illusion of Diminishing Returns: Measuring Long Horizon Execution in LLMs"
title_ja: "逓減効果の錯覚: LLMにおける長期実行の測定"
aliases: ["Illusion of Diminishing Returns", "Long Horizon Execution"]
tags: [paper, LLM, evaluation, long-horizon, execution, agents, reasoning]
status: note
venue: arXiv
year: 2025
authors:
  - Akshit Sinha
  - Arvindh Arun
  - Shashwat Goel
  - Steffen Staab
  - Jonas Geiping
arxiv: "2509.09677"
version: "v1"
links:
  - abs: https://arxiv.org/abs/2509.09677
  - pdf: https://arxiv.org/pdf/2509.09677v1.pdf
created: 2025-09-14
---

# The Illusion of Diminishing Returns: Measuring Long Horizon Execution in LLMs / 逓減効果の錯覚: LLMにおける長期実行の測定

## 概要（3行で）
- 目的: 単発精度の微小な差が長期タスクの「実行可能長」にどう効くかを、推論を切り離して「実行能力」に焦点化して測る。
- アプローチ: 知識と手順計画を提示し、実行のみを評価する長期ホライゾン設定とメトリクスを設計。モデル規模と逐次テスト時計算の効果も比較。
- 結果: 大規模化と逐次計算で実行可能長が大幅伸長。自己条件付け効果を観測する一方、Thinking系は当該効果を示さず単一ターン長期実行が強い。

## 主な貢献
- 逓減効果の“錯覚”の指摘: 単発精度の微増がタスク長で指数的差に化け得ることを定量的に示唆。
- 実行能力の分離評価: 知識・計画を与え、推論でなく実行に焦点化する計測プロトコルを提案。
- スケーリング利得の実証: モデル大規模化と逐次テスト時計算が実行可能長を大きく伸ばすことを示す。
- 自己条件付けの報告: エラー履歴を文脈に含むと誤りが増える効果を観測。単なるスケール拡大では解消困難。
- Thinkingモデルの特性: 自己条件付けを示さず、単一ターンの長期実行能力が高い。

## 手法 / 設計
- 問題設定: 難推論ではなく「同じ簡単な作業を長く続ける」長期ホライゾンの実行課題を評価対象にする。
- 提案手法: 必要な知識と手順計画を事前に提示し、実行の正確性のみを計測（[[words/long-horizon-execution]] の実行可能長など）。
- 実装要点: モデル規模を跨いで比較し、[[words/sequential-test-time-compute]] を増やす条件も評価。履歴中の誤り有無の条件で挙動を観測（[[words/self-conditioning]]）。

## 実験・評価（要約）
- セットアップ: 長い手順の遂行タスクを構築し、知識と計画を与えた状態で実行のみを評価。
- 比較条件: モデル規模の違い、逐次テスト時計算の有無、Thinking系モデルの挙動を比較。
- 指標: 実行可能長、ステップ長に対する単ステップ正答率の劣化、自己条件付け条件での誤り率変化。

## 主要結果
- 小規模モデルが単一ターンで満点な設定でも、長期実行では大規模モデルが大幅優勢。
- ステップが長くなると単ステップ精度が低下する“長さ依存の劣化”を観測。
- 履歴にエラーが残ると誤りが増える“自己条件付け”効果を観測。スケール拡大だけでは緩和しにくい。
- Thinking系モデルは自己条件付けを示さず、単一ターンの長期実行能力が高い。

## 考察・限界
- 位置づけ: 単発精度の逓減印象に反し、長期実行では利得が逓増的に現れる局面がある可能性を示す。
- 限界: 計画と知識を与えるため、探索・計画・ツール使用を含む現実タスク全体性能とは異なる指標。
- 将来課題: 自己条件付けの再現性検証、プロンプト設計の影響分析、ドメイン一般化とノイズ耐性の評価。

## 再現性メモ
- 依存/コード: 公開コードの記載はなし（ベンチマーク設計の要点が本文に示唆）。
- データ/環境: 長期実行タスク（反復的・ステップ化タスク）。
- ログ/設定: 履歴の扱い（エラー残留/非残留）、逐次テスト時計算の設定を明示。

## 応用・拡張アイデア
- エージェントの堅牢化: エラー連鎖を断つ訂正・再試行・履歴要約の戦略設計。
- 評価設計: 実運用に近い「長く続けるだけで難度が増す」評価の導入。

## キークォート
> “We start this work by observing the simple but counterintuitive fact that marginal gains in single-step accuracy can compound into exponential improvements in the length of a task a model can successfully complete.”

> “We observe a self-conditioning effect -- models become more likely to make mistakes when the context contains their errors from prior turns.”

## 用語メモ（words/ を使う）
- [[words/long-horizon-execution]]: 実行可能長や長期ホライゾンの実行能力に関する評価観点。
- [[words/self-conditioning]]: 履歴中の自エラーが後続誤りを誘発する現象。
- [[words/sequential-test-time-compute]]: 推論時に逐次・外部化された思考/検証を増やして精度を上げる手法群。
- [[words/thinking-models]]: 自己条件付けを示さず単一ターン長期実行に強いモデル群。

## 関連研究
- 長期ホライゾン評価、CoT/自己整合、エージェント実行の堅牢化、履歴要約・再計画などの自己修復戦略。

## BibTeX
```bibtex
@misc{sinha2025illusion,
  title         = {The Illusion of Diminishing Returns: Measuring Long Horizon Execution in LLMs},
  author        = {Akshit Sinha and Arvindh Arun and Shashwat Goel and Steffen Staab and Jonas Geiping},
  year          = {2025},
  eprint        = {2509.09677},
  archivePrefix = {arXiv},
  primaryClass  = {cs.AI},
  url           = {https://arxiv.org/abs/2509.09677}
}
```

## リンク
- arXiv: https://arxiv.org/abs/2509.09677
- PDF: https://arxiv.org/pdf/2509.09677v1.pdf
