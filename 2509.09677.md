---
title_en: "The Illusion of Diminishing Returns: Measuring Long Horizon Execution in LLMs"
title_ja: "逓減効果の錯覚: LLMにおける長期実行の測定"
aliases: ["Illusion of Diminishing Returns", "Long Horizon Execution"]
date: 2025-09-14
tags: [paper, LLM, evaluation, long-horizon, execution, agents, reasoning]
venue: arXiv
year: 2025
authors:
  - Akshit Sinha
  - Arvindh Arun
  - Shashwat Goel
  - Steffen Staab
  - Jonas Geiping
arxiv: "2509.09677"
version: "v1"
links:
  - abs: https://arxiv.org/abs/2509.09677
  - pdf: https://arxiv.org/pdf/2509.09677v1.pdf
---

# The Illusion of Diminishing Returns: Measuring Long Horizon Execution in LLMs / 逓減効果の錯覚: LLMにおける長期実行の測定

# 概要
- 「LLMのスケーリングは逓減するのか?」という問いに対し、単発（単一ターン）の正解率の“わずかな”改善が、長い手順を必要とするタスクの「実行可能長（失敗せずに進めるステップ数）」では指数的な差として現れることを指摘。
- 複雑さを増やすのではなく「同じ簡単な作業を長く続ける」状況での失敗は、主に“推論能力の欠如”ではなく“実行（execution）のミス”に起因すると主張。
- 知識と計画（plan）を明示的に与えて「実行能力」を切り出して測定する評価設定を提案し、モデル規模やテスト時の逐次計算（sequential test-time compute）拡張の有効性を示す。
- 大きなモデルは、小さなモデルが単一ターンで100%正解できる設定であっても、より多くのターンを正しく実行できる。
- ステップ数が増えるほど1ステップあたりの正確さが低下しやすいこと、また「自己条件付け（self-conditioning）」効果（自身の過去の誤りを文脈に含むと、さらに誤りやすくなる）を観測。この自己条件付けは単にモデルを大きくしても弱まらない。
- 一方で、近年の“thinking”系モデルは自己条件付けを示さず、長いタスクを単一ターンで実行できる長さが大きい。

# 貢献
- 逓減効果の“錯覚”の指摘: 単発精度の微増がタスク長では大差となり得るという直観に反する事実を定量的に示唆。
- 実行能力の分離評価: 知識・計画を事前に提示し、推論ではなく「実行」に焦点を当てる計測プロトコルを提案。
- 長期実行におけるスケーリング利得: 大規模化や逐次的なテスト時計算の拡張が、達成可能なタスク長を大きく伸ばすことを実証。
- 自己条件付けの発見と分析: エラー履歴が文脈に含まれるとさらなる誤りを誘発する現象を報告。モデルサイズ拡大だけでは十分に解消されない。
- Thinkingモデルの挙動: Thinkingアーキテクチャは自己条件付けを示さず、単一ターンでの長期実行能力が高いことを示すベンチマーク結果。

# 評価設定・メトリクス（要旨）
- タスク設計: “難しい推論”を課すのではなく、簡単な手順を長く続ける長期ホライゾン設定を構築。
- 実行能力の分離: 必要な知識とステップ化された計画をあらかじめ提示し、モデルは正確に実行するかを評価。
- 指標イメージ:
  - 実行可能長（何ターン連続で正しく実行できるか）
  - ステップ数に対する1ステップ正答率の劣化傾向
  - 自己条件付け下（過去誤りを文脈に残す）での誤り率変化

# 主な結果
- 小規模モデルが単一ターンで満点を取れる設定でも、長期実行では大規模モデルが大幅に優勢。
- ステップが長くなると1ステップ精度が下がる“長さ依存の劣化”を観測。
- エラーが文脈に残ると誤りが増える“自己条件付け”効果を観測。単なるモデルスケール拡大では緩和しにくい。
- Thinking系モデルは自己条件付けを示さず、単一ターンの長期実行能力が高い。

# 含意（インパクト）
- スケーリング利得の再評価: 単発精度だけを見ると逓減に見えても、長期実行では利得が“逓増”に見える場面がある。
- エージェント設計: 長期タスクでは“推論”よりも“実行の堅牢化（エラー連鎖の断ち切り）”が鍵。履歴の取り扱い・訂正戦略が重要。
- 評価設計: “長く続けるだけで難しくなる”タイプの評価を導入することで、実運用価値に近い能力差を可視化できる。
- テスト時計算: 逐次的・外部化された思考や検証（sequential test-time compute）の増加が長期実行性能に効く可能性。

# 限界・注意点（推察）
- 計画と知識を与えて“実行”に限定するため、現実のタスク（探索・計画・ツール使用を含む）全体性能とは異なる。
- 自己条件付けの強度や再現性はタスク設計・プロンプト設計に依存し得る。
- ベンチマークの一般化可能性（ドメインや長さの分布、外的ノイズ）には今後の検証が必要。

# 関連トピック
- 長期ホライゾン評価（長文コンテキスト、反復作業、エージェント実行）
- Thinking/CoT/自己整合・検証ステップの有効性
- エラー訂正・再計画・履歴要約などの自己修復戦略

# 引用・メタデータ
- タイトル: The Illusion of Diminishing Returns: Measuring Long Horizon Execution in LLMs
- 著者: Akshit Sinha; Arvindh Arun; Shashwat Goel; Steffen Staab; Jonas Geiping
- arXiv: 2509.09677 [cs.AI]
- 提出: 2025-09-11（arXiv表示）
- リンク: [abs](https://arxiv.org/abs/2509.09677), https://arxiv.org/pdf/2509.09677v1.pdf

# キークォート（抄）
> “We start this work by observing the simple but counterintuitive fact that marginal gains in single-step accuracy can compound into exponential improvements in the length of a task a model can successfully complete.”

> “We observe a self-conditioning effect -- models become more likely to make mistakes when the context contains their errors from prior turns.”

# BibTeX（簡易）
```bibtex
@misc{sinha2025illusion,
  title         = {The Illusion of Diminishing Returns: Measuring Long Horizon Execution in LLMs},
  author        = {Akshit Sinha and Arvindh Arun and Shashwat Goel and Steffen Staab and Jonas Geiping},
  year          = {2025},
  eprint        = {2509.09677},
  archivePrefix = {arXiv},
  primaryClass  = {cs.AI},
  url           = {https://arxiv.org/abs/2509.09677}
}
```

# メモ
- 実運用の長期エージェントを評価するうえで、「計画は与え、実行のみを測る」観点は現象の切り分けに有用。
- Thinking系の“単一ターン長期実行”能力は、長コンテキスト処理とは別のメカニズムで説明される可能性。
