---
title_en: "Narrative-Guided Reinforcement Learning: A Platform for Studying Language Model Influence on Decision Making"
title_ja: "ナラティブ誘導型強化学習: 言語モデルが意思決定に与える影響を研究するためのプラットフォーム"
aliases: ["Narrative-Guided RL", "NGRL"]
tags: [paper, RL, LLM, decision-making, narrative, architecture]
status: note
venue: arXiv
year: 2025
authors:
  - Anup Tuladhar
  - Araz Minhas
  - Adam Kirton
  - Eli Kinney-Lang
arxiv: "2509.08785"
version: "v1"
links:
  - abs: https://arxiv.org/abs/2509.08785
  - pdf: https://arxiv.org/pdf/2509.08785v1.pdf
created: {{date}}
---

# Narrative-Guided Reinforcement Learning: A Platform for Studying Language Model Influence on Decision Making / ナラティブ誘導型強化学習: 言語モデルが意思決定に与える影響を研究するためのプラットフォーム

# 概要（3行で）
- 目的: 強化学習（RL）とLLMの「ナラティブ」推論を統合した実験基盤を作り、物語的枠組みが報酬学習に与える影響を観察する。
- アプローチ: デュアルシステム構成（RLポリシーが行動案を提示し、LLMがナラティブ枠組みで解釈・誘導）をグリッドワールドで評価。
- 結果: 実装の実現可能性と基本ログ収集を示し、直接指示とキャラクター物語誘導で行動特性が異なる可能性を予備的に確認。

# 主な貢献
- RLとLLMを結合した「ナラティブ誘導」実験プラットフォームの提示（環境・報酬は一定のまま、意思決定誘導のみを操作）。
- モジュール式設計（環境複雑度・ナラティブパラメータ・RL/LLM相互作用を制御可能）。
- ログ基盤（RLポリシー値、LLM推論テキスト、選択行動のパターン）により、誘導の影響を追跡。

# 手法（Narrative-Guided RL）
- 問題設定: グリッドワールドでのナビゲーション。エージェントは観測と過去から学習したポリシーに基づき行動。
- デュアルシステム: RLポリシーが候補行動を提案（経験に基づく最適化）、LLMがナラティブ文脈で解釈し最終行動を選好・上書き。
- ナラティブの種類: 直接指示（タスク説明）と、キャラクターに基づく物語誘導（例: Theseus、Sherlock、WestworldのAIを想起させる役割）。
- 実装要点: 一貫した環境・報酬設計を維持しつつ、プロンプト（ナラティブ枠）とRL出力の結合のみを操作。図示例では訓練10/100エピソードのRLのみと、10エピソード時のRL+LLM（各ナラティブ）の軌跡を比較。
- 記録: RLの価値・方策スコア、LLMの推論内容、行動選択の時系列パターンを保存。

# 実験設定（要約）
- 環境: コンフィギュラブルなグリッドワールド。障害物や目標配置の複雑度を調整。
- 比較条件: RLのみ vs RL+LLM（直接指示／Theseus／Sherlock／Westworld誘導）。
- 学習: RLはエピソード学習（例示的に10/100エピソードの比較が図示）。
- 評価観点: 目標到達、経路の選好、障害物回避パターン、選択行動の多様性。ログから行動傾向を解析。

# 主要結果（予備）
- 行動特性: 直接指示と物語誘導で経路選好や回避挙動に差異が観察されるケースがある。環境構成を跨いでも一部パターンが持続。
- 有効性の兆候: 訓練が浅い局面（10エピソード）では、特定のナラティブが行動選好に形を与える可能性を示唆。
- 注意点: 結果は探索的であり、統計的な確証や再現性評価は未確立。

# 考察・限界
- 位置づけ: ナラティブが意思決定を形作るという認知科学の知見（デュアルプロセス理論）に基づき、RL最適化と象徴的推論の接合を試す第一歩。
- 限界: モデル規模、環境一般化、誘導一貫性、プロンプト設計の系統的比較が未完。より厳密なプロトコルが必要。
- 将来課題: モデルスケールの探索、小型〜大型LLMの比較、CoTやマルチエージェント推論、より洗練されたナラティブ設計、RLアルゴリズムの拡張検証。

# 再現性メモ（論文情報から）
- コード/依存: プラットフォームを構成するRLエージェントとLLM呼び出し、ログ収集機構が必要（詳細設定は本稿では概要のみ）。
- ログ: 方策値、LLM推論テキスト、実行行動、環境スナップショット。
- 環境: グリッドワールド（複雑度を制御）。

# 応用・拡張アイデア
- 安全性と誘導評価: ナラティブに起因する過剰確信／リスク選好の変化をメトリクス化。
- 報酬設計の補助: ナラティブを通じた「目標／制約」の内在化で探索を整形。
- 役割演技型プロンプト: 役割に応じた価値観・行動規範を組み込む比較（例: 探偵 vs 英雄 vs 倫理重視AI）。
- 反事実ナラティブ: 同一報酬の下で異なる物語誘導を対比し、意思決定の因果影響を同定。

# 重要引用（キークォート）
> "The system comprises a reinforcement learning policy that suggests actions based on past experience, and a language model that processes these suggestions through different narrative frameworks to guide decisions."
> "The platform's modular design facilitates controlled testing of environmental complexity, narrative parameters, and the interaction between reinforcement learning and narrative-based decisions."

# 用語メモ
- Narrative: 物語的枠組み（役割・価値観・行動規範）をプロンプトで与え、選択の解釈と優先度に影響させる手法。
- Dual-System: 経験に基づく即時最適化（RL）と、言語による高次推論（LLM）を分担させる構成。
- Guidance vs Control: 報酬・環境は固定し、行動選好の誘導（ガイダンス）のみを操作。

# 関連研究
- LLM as Planner/Judge: LLMを計画器・評価器として使う研究（例: LM as Zero-Shot Planner）。
- 誘導・説得・役割演技: プロンプトにより行動方針を変える研究潮流（Method-Acting的手法など）。
- 認知的枠組み: デュアルプロセス理論とRLの接合。

# BibTeX
```bibtex
@article{tuladhar2025narrative_guided_rl,
  title   = {Narrative-Guided Reinforcement Learning: A Platform for Studying Language Model Influence on Decision Making},
  author  = {Tuladhar, Anup and Minhas, Araz and Kirton, Adam and Kinney-Lang, Eli},
  journal = {arXiv preprint},
  year    = {2025},
  eprint  = {2509.08785},
  version = {v1}
}
```

# 補足（本文から読み取れる図・構成）
- 図1: グリッドワールドでの軌跡比較。RLのみ（10/100エピソード）と、RL+LLM（10エピソード、直接指示／Theseus／Sherlock／Westworld）。
- ログ: 行動選択の差や障害物回避の変化が確認できる例を提示。
